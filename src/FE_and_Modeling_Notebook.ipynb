{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "import shap\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.base import clone\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix, SCORERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df = pd.read_csv(\"../data/accidents.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering or encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_missing_val = ['End_Lat', 'End_Lng', 'Number', 'Precipitation(in)']\n",
    "zero_variance = ['Country', 'Turning_Loop']\n",
    "high_correlation = ['Wind_Chill(F)']\n",
    "text_feat = ['Description', 'Weather_Condition']#, 'Street'\n",
    "other = ['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols(df, col_set):\n",
    "    df1 = df.copy()\n",
    "    return df1.drop(columns = col_set)\n",
    "df1 = drop_cols(df, high_missing_val+zero_variance+high_correlation+text_feat+other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier Detection and Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wind Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_dir_dict = {'Calm': 'CALM',\n",
    "                'East': 'E',\n",
    "                'West': 'W',\n",
    "                'North': 'N',\n",
    "                'South': 'S',\n",
    "                'Variable': 'VAR'}\n",
    "def replace_vals(x):\n",
    "    if x in wind_dir_dict:\n",
    "        return wind_dir_dict[x]\n",
    "    else:\n",
    "        return x\n",
    "df1['Wind_Direction'] = df1['Wind_Direction'].apply(lambda x:replace_vals(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wind Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Wind_Speed(mph)'][df1['Wind_Speed(mph)']>231] = 231"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time of the day\n",
    "df1['St_HOD'] = df1['Start_Time'].apply(lambda x:int(x.split(' ')[-1].split(':')[0]))\n",
    "df1['Ed_HOD'] = df1['End_Time'].apply(lambda x:int(x.split(' ')[-1].split(':')[0]))\n",
    "# day of the week\n",
    "df1['Accident_DOW'] = pd.to_datetime(df1['Start_Time']).dt.dayofweek\n",
    "# week of the year\n",
    "df1['Accident_WOY'] = pd.to_datetime(df1['Start_Time']).dt.week\n",
    "# weekday/weekend - Although this is a correlated feature to Day of Week. We will let the algorithm choose the better of the two.\n",
    "df1['Weekday_Weekend'] = 0\n",
    "df1['Weekday_Weekend'][df1['Accident_DOW'].isin([5,6])] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Features - Street and Description\n",
    "# Ave/Avenue, Rd/Road, St/Street, Ln/Lane, Dr/Drive, Way, Pl/Plaza/Square, Blvd/boulevard, Hwy/Highway/Expy/Fwy/Parkway,\n",
    "# Main, Route\n",
    "street_dict = {\n",
    "    0: ['Ave', 'Avenue'],\n",
    "    1: ['Rd', 'Road'],\n",
    "    2: ['St', 'Street'],\n",
    "    3: ['Ln', 'Lane'],\n",
    "    4: ['Dr', 'Drive'],\n",
    "    5: ['Pl', 'Plaza', 'Square'],\n",
    "    6: ['Blvd', 'Boulevard'],\n",
    "    7: ['Hwy', 'Highway', 'Expy', 'Fwy', 'Parkway'],\n",
    "    8: ['Main'],\n",
    "    9: ['Route']\n",
    "}\n",
    "def find_street_type(string, street_dict):\n",
    "    for key in street_dict:\n",
    "        for tok in street_dict[key]:\n",
    "            if tok in string.split(' '):\n",
    "                return key\n",
    "    return 10\n",
    "df1['Street'] = df1['Street'].apply(lambda x:find_street_type(x, street_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Features: Accident Duration & Time between weather measurement and accident start time\n",
    "df1['Start_Time'] = pd.to_datetime(df1['Start_Time'])\n",
    "df1['End_Time'] = pd.to_datetime(df1['End_Time'])\n",
    "df1['Weather_Timestamp'] = pd.to_datetime(df1['Weather_Timestamp'])\n",
    "df1['Accident_Duration'] = df1['End_Time'] - df1['Start_Time']\n",
    "df1['Weather_Impact_Duration'] = df1['Start_Time'] - df1['Weather_Timestamp']\n",
    "df1 = df1.drop(columns = ['Start_Time', 'End_Time', 'Weather_Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location features\n",
    "# 'Start_Lat', 'Start_Lng', 'City', 'County', 'State', 'Zipcode', 'Timezone', 'Airport_Code'\n",
    "# In the first cut, I would select the 'State' feature and remove others mainly because it \n",
    "# has medium cardinality and zero nans\n",
    "df1 = df1[df1.columns.difference(['Start_Lat', 'Start_Lng', 'City', 'County', 'Zipcode', 'Timezone', 'Airport_Code'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Value Imputation and Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Categorical features\n",
    "bool_feat = df1.select_dtypes(include='bool').columns\n",
    "cat_feat = df1.select_dtypes(include='object').columns\n",
    "for col in cat_feat:\n",
    "    df1[col] = df1[col].fillna(df1[col].mode()[0])\n",
    "    u_val = df1[col].unique()\n",
    "    val_d = dict([(u_val[k], k) for k in range(len(u_val))])\n",
    "    df1[col] = df1[col].apply(lambda x:val_d[x])\n",
    "for col in bool_feat:\n",
    "    df1[col] = df1[col].apply(lambda x:int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Continuous Features with missing values\n",
    "## TODO: Add distribution to exploratory for confirming mean or median imputation\n",
    "con_feat = ['Humidity(%)', 'Pressure(in)', 'TMC', 'Temperature(F)', 'Visibility(mi)', 'Weather_Impact_Duration',\n",
    "           'Wind_Speed(mph)']\n",
    "for col in con_feat:\n",
    "    df1[col] = df1[col].fillna(df1[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Timestamp Features\n",
    "for col in ['Accident_Duration', 'Weather_Impact_Duration']:\n",
    "    df1[col] = df1[col].apply(lambda x:x.seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train-Test Split\n",
    "X = df1[[k for k in df1.columns if k!='Severity']]\n",
    "y = df1['Severity']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling for Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = X.copy()\n",
    "for col in X_scaled.columns:\n",
    "    X_scaled[col] = MinMaxScaler().fit_transform(np.array(X_scaled[col]).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation pipeline on training data for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Adjusting for imbalance in 3 common classifiers\n",
    "cls_wt_dict = dict(max(df1['Severity'].value_counts())/df1['Severity'].value_counts())\n",
    "f1_macro_score = {}\n",
    "clf_list = [LogisticRegression(class_weight=cls_wt_dict), DecisionTreeClassifier(class_weight=cls_wt_dict),\n",
    "            RandomForestClassifier(class_weight=cls_wt_dict)]\n",
    "for clf in clf_list:\n",
    "    scores = cross_val_score(clf, X_scaled, y, cv=5, scoring = 'f1_macro')\n",
    "    f1_macro_score[type(clf).__name__] = (np.mean(scores), np.std(scores))\n",
    "## Random Forest gives best performance out of the 3 simple classifiers\n",
    "f1_macro_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "feat_array = clf.feature_importances_\n",
    "feat_imp = pd.DataFrame([k for k in zip(X_train.columns, feat_array)]).sort_values(1, ascending=False).reset_index().iloc[:16,:][[0,1]]\n",
    "feat_imp.columns = ['Feature', 'Contribution']\n",
    "## Top 10 feature contribution\n",
    "print(feat_imp['Contribution'].sum())\n",
    "feat_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metric was choosen as 'F1 Macro' to make sure that all the classes irrespective of their support get equal weightage in the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check whether train and test belong from same distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numeric_cols = X_train.select_dtypes(include=numerics).columns\n",
    "def kstest(olddata, newdata, numeric_cols):\n",
    "    count_list = 0\n",
    "    same_list, diff_list = [], []\n",
    "    for col in numeric_cols:\n",
    "        oldvar = olddata[col]\n",
    "        newvar = newdata[col]\n",
    "        (s,p) = ks_2samp(oldvar.values.reshape((1,olddata.shape[0]))[0], newvar.values.reshape((1,newdata.shape[0]))[0])\n",
    "#         print(col, s, p)\n",
    "        if p > 0.05:\n",
    "            same_list.append(col)\n",
    "#             print(\"same\")\n",
    "            count_list += 1\n",
    "        else:\n",
    "            diff_list.append(col)\n",
    "    if count_list/len(numeric_cols) > 0.5:\n",
    "        \n",
    "        return 'same', same_list, diff_list\n",
    "    else:\n",
    "        return 'different', same_list, diff_list\n",
    "\n",
    "same_diff, same_list, diff_list = kstest(X_train, X_test, numeric_cols)\n",
    "print('Distribution:',same_diff)\n",
    "print('Columns with Same Distribution:',same_list)\n",
    "print('Columns with Different Distribution:',diff_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run test data through same pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "precision_recall_fscore_support(y_test, y_pred, average=None, labels=[1,2,3,4])\n",
    "# (array([0.61814496, 0.822673  , 0.58727317, 0.50919495]),\n",
    "#  array([0.63507603, 0.81590944, 0.59588685, 0.52772611]),\n",
    "#  array([0.62649612, 0.81927726, 0.59154866, 0.51829494]),\n",
    "#  array([  9602, 783560, 329237,  37095]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=0).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "precision_recall_fscore_support(y_test, y_pred, average=None, labels=[1,2,3,4])\n",
    "# (array([0.78085678, 0.81562352, 0.68786448, 0.70874232]),\n",
    "#  array([0.57519267, 0.89299096, 0.56534351, 0.45742014]),\n",
    "#  array([0.66242879, 0.85255561, 0.62061481, 0.55599974]),\n",
    "#  array([  9602, 783560, 329237,  37095]))\n",
    "# with class imbalance correction \n",
    "# (array([0.78518212, 0.80759026, 0.69159645, 0.72938191]),\n",
    "#  array([0.59268902, 0.89965159, 0.54047692, 0.43391293]),\n",
    "#  array([0.67548961, 0.85113877, 0.60676893, 0.54412386]),\n",
    "#  array([  9602, 783560, 329237,  37095]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## High run-time\n",
    "# clf = GradientBoostingClassifier(random_state=0).fit(X_train, y_train)\n",
    "# y_pred = clf.predict(X_test)\n",
    "# precision_recall_fscore_support(y_test, y_pred, average=None, labels=[1,2,3,4])\n",
    "# (array([0.76084462, 0.80717385, 0.65963367, 0.63435457]),\n",
    "#  array([0.34524057, 0.87432743, 0.576776  , 0.3164847 ]),\n",
    "#  array([0.47496239, 0.8394097 , 0.61542849, 0.42228697]),\n",
    "#  array([  9602, 783560, 329237,  37095]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Classifier (1 vs all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Very High run-time (Not recommended)\n",
    "# clf = OneVsRestClassifier(SVC()).fit(X_train, y_train)\n",
    "# y_pred = clf.predict(X_test)\n",
    "# precision_recall_fscore_support(y_test, y_pred, average=None, labels=[1,2,3,4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinal Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train - 1\n",
    "y_test = y_test - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinalClassifier():\n",
    "    \n",
    "    def __init__(self, clf):\n",
    "        self.clf = clf\n",
    "        self.clfs = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.unique_class = np.sort(np.unique(y))\n",
    "        if self.unique_class.shape[0] > 2:\n",
    "            for i in range(self.unique_class.shape[0]-1):\n",
    "                # for each k - 1 ordinal value we fit a binary classification problem\n",
    "                binary_y = (y > self.unique_class[i]).astype(np.uint8)\n",
    "                clf = clone(self.clf)\n",
    "                clf.fit(X, binary_y)\n",
    "                self.clfs[i] = clf\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        clfs_predict = {k:self.clfs[k].predict_proba(X) for k in self.clfs}\n",
    "        predicted = []\n",
    "        for i,y in enumerate(self.unique_class):\n",
    "            if i == 0:\n",
    "                # V1 = 1 - Pr(y > V1)\n",
    "                predicted.append(1 - clfs_predict[y][:,1])\n",
    "            elif y in clfs_predict:\n",
    "                # Vi = Pr(y > Vi-1) - Pr(y > Vi)\n",
    "                 predicted.append(clfs_predict[y-1][:,1] - clfs_predict[y][:,1])\n",
    "            else:\n",
    "                # Vk = Pr(y > Vk-1)\n",
    "                predicted.append(clfs_predict[y-1][:,1])\n",
    "        return np.vstack(predicted).T\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OrdinalClassifier(DecisionTreeClassifier())\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "precision_recall_fscore_support(y_test, y_pred, average=None, labels=[1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OrdinalClassifier(RandomForestClassifier(random_state=0))\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "precision_recall_fscore_support(y_test, y_pred, average=None, labels=[1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Power Analysis:\n",
    "1. Improving train speed\n",
    "2. Correcting imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Undersampling Class 2 and 3 to match class 1 numbers\n",
    "## Here we are emphasising on improving class 4 Recall.\n",
    "## it really depends on the business case i.e. if a govt agency wants to ensure avoiding severity 4 traffic jams\n",
    "sample_size = df1[df1['Severity']==1].shape[0]\n",
    "df1_1 = df1[df1['Severity']==1]\n",
    "df1_2 = df1[df1['Severity']==2].sample(sample_size)\n",
    "df1_3 = df1[df1['Severity']==3].sample(sample_size)\n",
    "df1_4 = df1[df1['Severity']==4]\n",
    "df2 = pd.concat([df1_1, df1_2, df1_3, df1_4])\n",
    "X_train = df2[[k for k in df2.columns if k!='Severity']]\n",
    "y_train = df2['Severity']\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "precision_recall_fscore_support(y_test, y_pred, average=None, labels=[1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that we got 99%+ Recall on Class 4, but pretty poor Precision.\n",
    "But is this information enough for the govt agency to avoid Severity 4 instances?\n",
    "The answer is NO.\n",
    "They need to know their action items which we would explain by SHAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Picking most confident examples\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "prob_mat = clf.predict_proba(X_test)\n",
    "idx1 = np.argmax([k[0] for k in prob_mat])\n",
    "idx2 = np.argmax([k[1] for k in prob_mat])\n",
    "idx3 = np.argmax([k[2] for k in prob_mat])\n",
    "idx4 = np.argmax([k[3] for k in prob_mat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHAP Example 1\n",
    "explainer = shap.TreeExplainer(clf)\n",
    "choosen_instance = X_test.loc[[idx1]]\n",
    "shap_values = explainer.shap_values(choosen_instance)\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], choosen_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHAP Example 2\n",
    "explainer = shap.TreeExplainer(clf)\n",
    "choosen_instance = X_test.loc[[idx2]]\n",
    "shap_values = explainer.shap_values(choosen_instance)\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], choosen_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHAP Example 3\n",
    "explainer = shap.TreeExplainer(clf)\n",
    "choosen_instance = X_test.loc[[idx3]]\n",
    "shap_values = explainer.shap_values(choosen_instance)\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], choosen_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHAP Example 4\n",
    "explainer = shap.TreeExplainer(clf)\n",
    "choosen_instance = X_test.loc[[idx4]]\n",
    "shap_values = explainer.shap_values(choosen_instance)\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], choosen_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Class 3 is Severity 4\n",
    "shap.summary_plot(shap_values, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretibility\n",
    "### Supply the index of specific test row\n",
    "train_cols = X_train.columns\n",
    "row_idx = idx2\n",
    "max_display  = 1200\n",
    "data_for_prediction = X_test.loc[row_idx]\n",
    "data_for_prediction = np.array(data_for_prediction).reshape(1,-1)\n",
    "# Calculate Shap values\n",
    "explainer = shap.TreeExplainer(clf)\n",
    "shap_values = explainer.shap_values(data_for_prediction)\n",
    "\n",
    "pred_probability = clf.predict_proba(data_for_prediction)\n",
    "prediction = np.argmax(pred_probability)\n",
    "print(\"prediction: \", prediction, pred_probability)\n",
    "\n",
    "shap_value = shap_values[prediction]\n",
    "\n",
    "feature_order = np.argsort(np.sum(np.abs(shap_value), axis=0))\n",
    "feature_order = np.flip(feature_order[-min(max_display, len(feature_order)):], 0)\n",
    "\n",
    "top_shape_vals = [shap_value[0][i] for i in feature_order]\n",
    "\n",
    "top_feature_names = [train_cols[i] for i in feature_order]\n",
    "\n",
    "fig = plt.figure(figsize=(18,15))\n",
    "y_pos = np.arange(len(top_feature_names))\n",
    "\n",
    "values = np.flip(top_shape_vals)\n",
    "features = np.flip(top_feature_names)\n",
    "\n",
    "plt.barh(y_pos, values, align='center')\n",
    "plt.yticks(y_pos, fontsize=15)\n",
    "plt.gca().set_yticklabels(features)\n",
    "plt.title(\"Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = list(feat_imp.Feature)\n",
    "X_train = df1[important_features]\n",
    "y_train = df1['Severity']\n",
    "dt_clf = DecisionTreeClassifier(random_state=0, max_depth = 3).fit(X_train, y_train)\n",
    "tree.plot_tree(dt_clf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
