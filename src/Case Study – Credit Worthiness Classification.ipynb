{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Data download and read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Necessary imports\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "# sorted(metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data download, Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_credit = pd.read_csv(\"german_credit.csv\")\n",
    "y = df_credit['Risk']\n",
    "X = df_credit.drop(columns = ['Risk'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.1)\n",
    "\n",
    "df_train = pd.concat([X_train, y_train], axis = 1)\n",
    "df_test = pd.concat([X_test, y_test], axis = 1)\n",
    "\n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Test Distribution comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:, 'Credit History'].hist(alpha=0.5, label='Train', density=True)    \n",
    "df_test.loc[:, 'Credit History'].hist(alpha=0.5, label='Test', density=True)\n",
    "plt.xlabel('Credit History')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:, 'Age'].hist(alpha=0.5, label='Train', density=True)    \n",
    "df_test.loc[:, 'Age'].hist(alpha=0.5, label='Test', density=True)  \n",
    "plt.xlabel('Age')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:, 'Job'].hist(alpha=0.5, label='Train', density=True)    \n",
    "df_test.loc[:, 'Job'].hist(alpha=0.5, label='Test', density=True)\n",
    "plt.xlabel('Job')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:, 'Duration'].hist(alpha=0.5, label='Train', density=True)    \n",
    "df_test.loc[:, 'Duration'].hist(alpha=0.5, label='Test', density=True)\n",
    "plt.xlabel('Duration')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:, 'Credit amount'].hist(alpha=0.5, label='Train', density=True)    \n",
    "df_test.loc[:, 'Credit amount'].hist(alpha=0.5, label='Test', density=True)\n",
    "plt.xlabel('Credit amount')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:, 'Saving accounts'].hist(alpha=0.5, label='Train', density=True)    \n",
    "df_test.loc[:, 'Saving accounts'].hist(alpha=0.5, label='Test', density=True)\n",
    "plt.xlabel('Saving accounts')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "ks_2samp(df_train['Age'], df_test['Age'])\n",
    "ks_2samp(df_train['Credit amount'], df_test['Credit amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.Risk.value_counts() ### Good = 1 (credit worthy), Bad = 0 (not worthy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.Risk.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_summ = df_train.describe()\n",
    "df_train_summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_train['Credit amount'])\n",
    "plt.title('Credit amount distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the credit amount is highly skewed distribution, lets analyse the extreme values beyond 3 sigma\n",
    "def extreme_count(sig_factor, feat):\n",
    "    sig_cutoff = df_train_summ[feat]['mean'] + sig_factor*df_train_summ[feat]['std'] \n",
    "    sig_count = len(df_train[df_train[feat] > sig_cutoff])\n",
    "    print(\"instances of {} greater than {} sigma ({} cutoff) are {}\".format(feat, sig_factor, sig_cutoff, sig_count))\n",
    "    return\n",
    "\n",
    "extreme_count(3, feat = 'Credit amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_train['Age'])\n",
    "plt.title('Age distribution')\n",
    "extreme_count(3, feat = 'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_train['Duration'])\n",
    "plt.title('Duration distribution')\n",
    "extreme_count(3, feat = 'Duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Even if there are certain instances where the above features are beyond 3sigma of their mean value, they dont appear to be \n",
    "outliers, as its legible to have certain certain loans with high credit value, or loan duration is longer, or older population \n",
    "is seeking loan. Hence, I am not eliminating these rows'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Missing values, checking if they are legitimate and applying apt transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NaN is a valid field here implying no saving account\n",
    "\n",
    "df_train['Saving accounts'].value_counts()\n",
    "df_train['Saving accounts'].unique()\n",
    "\n",
    "### So, replacing NaN with 'no account'\n",
    "df_train.loc[df_train['Saving accounts'].isnull(), 'Saving accounts'] = 'no account'\n",
    "\n",
    "### Replaced in df\n",
    "df_train['Saving accounts'].value_counts()\n",
    "df_train['Saving accounts'].unique()\n",
    "\n",
    "### No NaNs anymore\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking data types and categorical states of features for encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection for label and one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_count = (pd.DataFrame(df_train.nunique(), columns=['count'])).reset_index()\n",
    "#le_list = df_count[df_count['count'] == 2]['index'].values.tolist()\n",
    "df_dtypes = pd.DataFrame((df_credit.dtypes == 'object'), columns = ['obj_type'])\n",
    "obj_list = df_dtypes[(df_dtypes.obj_type == True)].index\n",
    "#print(obj_list)\n",
    "#ohe_list = list(obj_list.difference(le_list))\n",
    "print(\"Features for label encoding:\", obj_list)\n",
    "#print(\"Features for OHE:\", ohe_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_obj = LabelEncoder()\n",
    "for feat in obj_list:\n",
    "    df_train[feat] = le_obj.fit_transform(df_train[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Risk variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.Risk.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "279/621"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "g = sns.pairplot(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.corr()\n",
    "### Credit amount and Duration have high correlation relatively and can be considered in feature selection step to drop 'Duration'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ques1 : More credit history is equivalent to credit worthiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([df_train.loc[df_train['Risk'] == 0, 'Credit History'].values, df_train.loc[df_train['Risk'] == 1, 'Credit History'].values], alpha=0.5, label=['Bad Risk', 'Good Risk'])\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['Risk'] == 0]['Credit History'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['Risk'] == 1]['Credit History'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: As the credit history increases, the good risk increases proportionately i..e credit worthiness improves sharply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques2 : Are young people more credit worthy?\n",
    "Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([df_train.loc[df_train['Risk'] == 0, 'Age'].values, df_train.loc[df_train['Risk'] == 1, 'Age'].values], alpha=0.5, label=['Bad Risk', 'Good Risk'])\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.Age[df_train.Age <= 30] = 0\n",
    "df_train.Age[(df_train.Age > 30) & (df_train.Age < 45)] = 1\n",
    "df_train.Age[(df_train.Age >= 45)] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['Risk'] == 0]['Age'].value_counts()\n",
    "df_train[df_train['Risk'] == 1]['Age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More credit accounts equivalent to more credit worthy?\n",
    "Inconclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([df_train.loc[df_train['Risk'] == 0, 'Saving accounts'].values, df_train.loc[df_train['Risk'] == 1, 'Saving accounts'].values], alpha=0.5, label=['Bad Risk', 'Good Risk'])\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['Risk'] == 0]['Saving accounts'].value_counts()\n",
    "df_train[df_train['Risk'] == 1]['Saving accounts'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train, y_train prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['Risk']\n",
    "X_train = df_train.drop(columns = ['Risk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum()\n",
    "### So, replacing NaN with 'no account'\n",
    "df_test.loc[df_test['Saving accounts'].isnull(), 'Saving accounts'] = 'no account'\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_obj = LabelEncoder()\n",
    "for feat in obj_list:\n",
    "    df_test[feat] = le_obj.fit_transform(df_test[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = df_train.columns.tolist()\n",
    "test_cols = df_test.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_cols = list(set(train_cols).difference(test_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_test['Risk']\n",
    "X_test = df_test.drop(columns = ['Risk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=1/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### As it is bad to classify a customer as good when they are bad i.e. objective is to reduce FP, we want better precision \n",
    "### Hence, applying beta = 1/5 and selecting fbeta_score as evaluation metric\n",
    "fbeta_mu_score, fbeta_sigma_score = [], []\n",
    "model_list = [DecisionTreeClassifier(), RandomForestClassifier(), LogisticRegression()]\n",
    "for model in model_list:\n",
    "    obj = model\n",
    "    scores = cross_val_score(obj, X_train, y_train, cv=5, scoring = ftwo_scorer)\n",
    "    fbeta_mu_score.append(np.mean(scores))\n",
    "    fbeta_sigma_score.append(np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbeta_mu_score\n",
    "fbeta_sigma_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_index = fbeta_mu_score.index(max(fbeta_mu_score))\n",
    "selected_model = model_list[model_index]\n",
    "selected_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = RandomForestClassifier(max_depth=3, min_samples_leaf=50)\n",
    "selected_model.fit(X_train, y_train)\n",
    "y_pred = selected_model.predict(X_test)\n",
    "fbeta_score(y_test, y_pred, beta=1/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.DataFrame()\n",
    "df_feat['cols'] = X_train.columns\n",
    "if str(selected_model)[:3] == 'Log' :\n",
    "    df_feat['importance'] = np.abs(selected_model.coef_[0])\n",
    "else:\n",
    "    df_feat['importance'] = selected_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3 = list(df_feat.sort_values(by = 'importance', ascending = False).head(3)['cols'].values)\n",
    "print(\"top 3 features:\", top_3)\n",
    "print(\"Top 3 features' cumulative importance:\", np.round(100*df_feat.sort_values(by = 'importance', ascending = False).head(3)['importance'].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretibility\n",
    "### Supply the index of specific test row\n",
    "\n",
    "row_idx = 589\n",
    "max_display  = 300\n",
    "data_for_prediction = X_test.loc[row_idx]\n",
    "data_for_prediction = np.array(data_for_prediction).reshape(1,-1)\n",
    "# Calculate Shap values\n",
    "explainer = shap.TreeExplainer(selected_model)\n",
    "shap_values = explainer.shap_values(data_for_prediction)\n",
    "\n",
    "pred_probability = selected_model.predict_proba(data_for_prediction)\n",
    "prediction = np.argmax(pred_probability)\n",
    "print(\"prediction: \", prediction, pred_probability)\n",
    "\n",
    "shap_value = shap_values[prediction]\n",
    "\n",
    "feature_order = np.argsort(np.sum(np.abs(shap_value), axis=0))\n",
    "feature_order = np.flip(feature_order[-min(max_display, len(feature_order)):], 0)\n",
    "\n",
    "top_shape_vals = [shap_value[0][i] for i in feature_order]\n",
    "\n",
    "top_feature_names = [train_cols[i] for i in feature_order]\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "y_pos = np.arange(len(top_feature_names))\n",
    "\n",
    "values = np.flip(top_shape_vals)\n",
    "features = np.flip(top_feature_names)\n",
    "\n",
    "plt.barh(y_pos, values, 0.7, align='center')\n",
    "plt.yticks(y_pos, fontsize=13)\n",
    "plt.gca().set_yticklabels(features)\n",
    "plt.title(\"Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,5))\n",
    "y_pos = np.arange(len(top_feature_names))\n",
    "\n",
    "values = np.flip(top_shape_vals)\n",
    "features = np.flip(top_feature_names)\n",
    "\n",
    "plt.barh(y_pos, values, 0.7, align='center')\n",
    "plt.yticks(y_pos, fontsize=13)\n",
    "plt.gca().set_yticklabels(features)\n",
    "plt.title(\"Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.loc[881,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roughwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Downloaded the data from the given link, copied the contents in a csv and removed the last 4 columns for which there wasnt\n",
    "any description. Drafted cols_list to give the header to the dataframe''' \n",
    "\n",
    "cols_list = ['Existing_Acc', 'Dur_Month', 'Credit_History', 'Credit_Purpose', 'Credit_Amt', 'Svgs', 'Employed_since', 'I_to_I', 'Gender', 'Guarantors', 'Residence_since', 'Property', 'Age', 'Installment_Plans', 'Housing', 'Existing_Credits', 'Job', 'Liable_Cnt', 'Phone', 'Foreign_Worker', 'Worthy']\n",
    "df = pd.read_csv(\"German_data.csv\", header=None)\n",
    "df.columns = cols_list\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Confirming the states of each attribute with the data description file for consistency check\n",
    "[(k, df[k].unique()) for k in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Job has 2 unique vals, while description says 4\n",
    "### People being libale is just 2 states, doesnt look apt\n",
    "### Existing credits, Instalment plans and housing -- same reason\n",
    "### Guarantors is Age column\n",
    "### Residence_since is Credit_Purpose\n",
    "\n",
    "cols_selected_till_now = ['Age', 'Property', 'Phone', 'Foreign_Worker', 'Worthy']\n",
    "\n",
    "### Based on discrepancies arising between the column attributes and names, as the file downloaded didnt have col names mapped strcuture\n",
    "### Checked for csv form data and found this csv. Will be using this for the rest of the analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
